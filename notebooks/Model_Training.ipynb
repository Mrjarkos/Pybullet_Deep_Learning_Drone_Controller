{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relevant-director",
   "metadata": {},
   "source": [
    "## Model Train 1 - Tesis Javier-Uriel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-dodge",
   "metadata": {},
   "source": [
    "### Importamos algunas librerías que nos serán útiles más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formal-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "assert (tf.__version__=='2.4.1'), 'Versión incorrecta de Tensorflow, por favor instale 2.4.1'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "pd.set_option('display.max_columns', None) #Para mostrar todas las columnas\n",
    "\n",
    "import gc #garbage collector\n",
    "import gc; gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moving-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-generic",
   "metadata": {},
   "source": [
    "### Leemos el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "undefined-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset solo movimientos en Z\n",
    "rpm_list = ['RPM0', 'RPM1', 'RPM2', 'RPM3']\n",
    "states_list_org = [\"vz\",\"az\", \"uvz\", \n",
    "                    \"p\", \"q\",\n",
    "                    \"wp\", \"wq\", \n",
    "                    \"ap\", \"aq\"]\n",
    "dataset_name = \"/Dataset_Z10_Alle\"\n",
    "directory = \"../logs/Datasets\"\n",
    "ORDER = 3\n",
    "states_list=states_list_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contrary-fairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ../logs/Datasets_train/Dataset_Z10_Alle \n"
     ]
    }
   ],
   "source": [
    "path = directory+\"_train\"+dataset_name\n",
    "if not os.path.exists(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "else:\n",
    "    print(f\"{path} already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "multiple-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_read():\n",
    "    dfs = []\n",
    "    global states_list\n",
    "    # reading train data\n",
    "    for filename in os.listdir(directory+dataset_name):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(directory+dataset_name, filename))\n",
    "        a = []\n",
    "        ## Desplazamos estados anteriores        \n",
    "        for n in range(1,ORDER+1):\n",
    "            for column in states_list:\n",
    "                df[column+str(n)] = df[column].shift(periods=n, fill_value=0)\n",
    "                a.append(column+str(n))\n",
    "        dfs.append(df)\n",
    "    states_list+=a       \n",
    "\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "backed-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    dataset = pandas_read()\n",
    "else:\n",
    "    for column in states_list_org:\n",
    "        for n in range(1,ORDER+1):\n",
    "            states_list.append(column+str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-dating",
   "metadata": {},
   "source": [
    "### Estados repetidos\n",
    "\n",
    "En este caso se eliminan estados repetidos y estados que se encuentren en estado transitorio mientras el dron despega o se estabiliza antes de introducir la señal de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    shape_b4 = dataset.shape\n",
    "    dataset = dataset.drop([\"timestamps\"], axis=1).drop_duplicates()\n",
    "    shape_drop = dataset.shape\n",
    "    print(f'shape_drop={shape_drop}')\n",
    "    print(f'len (b4 drop) - len = {shape_b4[0]-shape_drop[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-administrator",
   "metadata": {},
   "source": [
    "### División del dataset en estados y acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    actions = dataset[rpm_list]\n",
    "    actions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-expression",
   "metadata": {},
   "source": [
    "#### Normalización de acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df, K=21666.4475, B=14468.4292):\n",
    "    df_norm = (actions-B)/K\n",
    "    return df_norm, K, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    actions, K, B = normalize_df(actions)\n",
    "    dataset = dataset.drop(columns=rpm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-charm",
   "metadata": {},
   "source": [
    "#### Definimos los estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    states = dataset[states_list]\n",
    "    print(f'columns = {states.columns}')\n",
    "    print(f'shape = {states.shape}')\n",
    "    dataset = pd.concat([states, actions], axis=1)\n",
    "    #states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-treatment",
   "metadata": {},
   "source": [
    "#### Guardamos el dataset en un nuevo archivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(path+\"/train\"+\".csv\"):\n",
    "    dataset_test = dataset.sample(frac =.10, random_state = 10)\n",
    "    dataset_test.to_csv(path+\"/test.csv\", index=False)\n",
    "    dataset = pd.merge(dataset,dataset_test, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "    del dataset_test\n",
    "    dataset_val = dataset.sample(frac =.20, random_state = 10)\n",
    "    dataset_val.to_csv(path+\"/validation.csv\", index=False)\n",
    "    dataset = pd.merge(dataset,dataset_val, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "    del dataset_val\n",
    "    dataset.sample(frac =1, random_state = 42).to_csv(path+\"/train.csv\", index=False)\n",
    "    del dataset\n",
    "    del states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-owner",
   "metadata": {},
   "source": [
    "### Creamos dataset de Tensorflow a partir de csv\n",
    "Esto es con el fin de no cargar todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YieldReadCSV:\n",
    "    def __init__(self, file_path=path, file=\"/train.csv\", skip_rows=1, batch_size=512):\n",
    "        self.file_path = file_path\n",
    "        self.file = file\n",
    "        self.skip_rows = skip_rows\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def read_csv(self):\n",
    "        inputs = []\n",
    "        batchcount = 0\n",
    "        with open(self.file_path+self.file, 'r') as csvfile:\n",
    "            data = csv.reader(csvfile, delimiter=',')\n",
    "            for index, row in enumerate(data):\n",
    "                if index < self.skip_rows:\n",
    "                    continue\n",
    "                batchcount += 1\n",
    "                inputs.append([float(i) for i in row])\n",
    "                if batchcount >= self.batch_size:\n",
    "                    yield inputs\n",
    "                    inputs = []\n",
    "                    batchcount = 0\n",
    "            \n",
    "def XY_split(x):\n",
    "    labels = x[:,len(states_list):]\n",
    "    features = x[:, :len(states_list)]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN = 1024\n",
    "BATCH_SIZE_VAL = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = YieldReadCSV(file=\"/train.csv\", batch_size = BATCH_SIZE_TRAIN)\n",
    "dataset_train = tf.data.Dataset.from_generator(train_generator.read_csv,\n",
    "                                        output_types = tf.float32,\n",
    "                                        output_shapes = (None,None,))\n",
    "dataset_train = dataset_train.map(XY_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = YieldReadCSV(file=\"/validation.csv\",  batch_size = BATCH_SIZE_VAL)\n",
    "dataset_val = tf.data.Dataset.from_generator(val_generator.read_csv,\n",
    "                                        output_types=tf.float32,\n",
    "                                        output_shapes = (None,None,))\n",
    "dataset_val = dataset_val.map(XY_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = YieldReadCSV(file=\"/test.csv\", batch_size = BATCH_SIZE_VAL)\n",
    "dataset_test = tf.data.Dataset.from_generator(test_generator.read_csv,\n",
    "                                        output_types=tf.float32,\n",
    "                                        output_shapes = (None,None,))\n",
    "dataset_test = dataset_test.map(XY_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-nowhere",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-clear",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-stamp",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early_Stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-publicity",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-neighborhood",
   "metadata": {},
   "source": [
    "#### Definición del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(len(states_list),), name='inputs')\n",
    "x = tf.keras.layers.Dense(4*len(states_list), activation=tf.nn.relu)(inputs)\n",
    "x = tf.keras.layers.Dense(2*len(states_list), activation=tf.nn.relu)(x)\n",
    "outputs = tf.keras.layers.Dense(len(rpm_list), activation=tf.nn.tanh)(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-witness",
   "metadata": {},
   "source": [
    "#### Compilado el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['mean_squared_error']\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0025)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-keeping",
   "metadata": {},
   "source": [
    "#### Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LEN_DT = 200000\n",
    "EPOCHS =  250\n",
    "\n",
    "history = model.fit(dataset_train.repeat(), \n",
    "                    epochs=EPOCHS, \n",
    "                    steps_per_epoch = LEN_DT//BATCH_SIZE_TRAIN,\n",
    "                    callbacks=[Early_Stopping, plot_losses], \n",
    "                    verbose=1,\n",
    "                    validation_data = dataset_val.repeat(),\n",
    "                    validation_steps= (LEN_DT*0.2)//BATCH_SIZE_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-differential",
   "metadata": {},
   "source": [
    "#### Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mean_sq = model.evaluate(dataset_test, \n",
    "                               steps = LEN_DT//BATCH_SIZE_VAL\n",
    "                              )\n",
    "K = 21666.4475 #Ganancia del actuador\n",
    "B = 14468.4292\n",
    "print(f'mean_sq: {mean_sq} -> {(mean_sq)*K} RPM')\n",
    "print(f'loss: {loss} -> {loss*K} RPM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-photography",
   "metadata": {},
   "source": [
    "#### Se guarda el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 2\n",
    "model.save(f'../Models/{dataset_name}_{I}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'../Models/{dataset_name}_{I}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_test = [0]*len(states_list)\n",
    "x_test[0] = 0\n",
    "x_test[1] = 0\n",
    "x_test[2] = 0\n",
    "print(model.predict([list(x_test)]))\n",
    "print(model.predict([list(x_test)])*K+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for index, sample in dataset_test.take(1):\n",
    "#     for i in sample:\n",
    "#         print(model.predict([list(i)])*K['K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-press",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
